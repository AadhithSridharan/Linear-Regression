import numpy as np
import matplotlib.pyplot as plt
from utils import *
import copy
import math

def compute_cost(x, y, w, b): 
    m = len(x)
    total_cost = 0
    tempcost=0
    for i in range(m):
        tempcost+=(w*x[i]+b-y[i])**2
    total_cost=tempcost/(m*2)
    return total_cost

def compute_gradient(x, y, w, b):
    m = len(x)
    dj_dw = 0
    dj_db = 0
    tempjb=tempjw=0
    for i in range(m):
        tempjb,tempjw=tempjb+w*x[i]+b-y[i],tempjw+x[i]*(w*x[i]+b-y[i])
    dj_dw,dj_db=tempjw/m,tempjb/m 
        
    return dj_dw, dj_db

def gradient_descent(x, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters): 
    m = len(x)
    J_history = []
    w_history = []
    w = copy.deepcopy(w_in)
    b = b_in
    for i in range(num_iters):
        dj_dw, dj_db = gradient_function(x, y, w, b )
        w = w - alpha * dj_dw
        b = b - alpha * dj_db               

        if i<100000:
            cost =  cost_function(x, y, w, b)
            J_history.append(cost)
        if i% math.ceil(num_iters/10) == 0:
            w_history.append(w)
            print(f"Iteration {i:4}: Cost {float(J_history[-1]):8.2f}   ")
    return w, b, J_history, w_history

x_train,y_train=[],[]
print("Enter training data input format (1 or 2): \n1) Consecutive pairs of x and y \n2) List of x and list of y")
entry=int(input())
m=int(input("Enter number of training samples:"))
if entry==1:
    for i in range(m):
        x_train.append(float(input(f"x{i+1}: ")))
        y_train.append(float(input(f"y{i+1}: ")))
elif entry==2:
    for i in range(m):
        x_train.append(float(input(f"x{i+1}: ")))
    for i in range(m):
        y_train.append(float(input(f"y{i+1}: ")))
else:
    print("Invalid entry")

mx,my=len(x_train),len(y_train)
plt.scatter(x_train, y_train, marker='x', c='r')
plt.title("Training data")
plt.ylabel('y axis')
plt.xlabel('x axis')
plt.show()

initial_w = 2
initial_b = 1
cost = compute_cost(x_train, y_train, initial_w, initial_b)
print(f'Cost at initial w: {cost:.3f}')

initial_w = 0
initial_b = 0
tmp_dj_dw, tmp_dj_db = compute_gradient(x_train, y_train, initial_w, initial_b)
print('Gradient at initial w, b (zeros):', tmp_dj_dw, tmp_dj_db)

initial_w = 0.
initial_b = 0.
iterations = 500
alpha = 0.01

w,b,_,_ = gradient_descent(x_train ,y_train, initial_w, initial_b, compute_cost, compute_gradient, alpha, iterations)
print("w,b found by gradient descent:", w, b)

predicted = np.zeros(m)
for i in range(m):
    predicted[i] = w * x_train[i] + b

plt.plot(x_train, predicted, c = "b")

plt.scatter(x_train, y_train, marker='x', c='r')
plt.ylabel('y axis')
plt.xlabel('x axis')
plt.show()

x_op=float(input("Input value: "))
print("Predicted output value for input",x_op," = ",x_op*w+b)
